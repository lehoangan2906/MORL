{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5gtn6zP+aY871NCCMyLTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehoangan2906/MORL/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHr-gfp8ZKL9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyexpat import model\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import gym \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiQNetwork(keras.Model):\n",
        "  \"\"\"\n",
        "  Xây dựng Multi Q Network dựa trên 4 Fully Connected + Các tham chiếu w\n",
        "  Đầu vào:\n",
        "  Concatenation of State representation and parameters of a linear preferrence function\n",
        "  Đầu ra:\n",
        "  Giá trị Q-network với kích thước m*size|A| (m = reward_size: số mục tiêu, A: action_space)\n",
        "  \"\"\"\n",
        "  def __init__(self, state_size, action_size, reward_size):\n",
        "    super(MultiQNetwork, self).__init__()\n",
        "\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.reward_size = reward_size\n",
        "\n",
        "    input_shape = state_size + reward_size\n",
        "    action_space = action_size * reward_size\n",
        "\n",
        "    self.layer1 = Dense(input_shape * 16, activation = 'relu', kernel_initializer = 'he_uniform')\n",
        "    self.layer2 = Dense(input_shape * 32, activation = 'relu', kernel_initializer = 'he_uniform')\n",
        "    self.layer3 = Dense(input_shape * 64, activation = 'relu', kernel_initializer = 'he_uniform')\n",
        "    self.layer4 = Dense(input_shape * 32, activation = 'relu', kernel_initializer = 'he_uniform')\n",
        "    self.layer5 = Dense(action_space, activation = 'relu', kernel_initializer = 'he_uniform')\n",
        "\n",
        "\n",
        "  def H(self, Q, w, s_num, w_num):\n",
        "    \"\"\"\n",
        "    Toán tử HQ\n",
        "    w_num: Số lượng tham chiếu lấy mẫu ra từ D_w\n",
        "    S_num: Số lượng state\n",
        "    \"\"\"\n",
        "    mask = tf.concat([np.arange(i, s_num * w_num + i, s_num) for i in range(s_num)], axis = 0)\n",
        "    test = tf.reshape(Q, [-1, self.action_size * self.reward_size])\n",
        "\n",
        "    TT = []\n",
        "    for i in range(len(mask)):\n",
        "      TT.append(test[mask[i]])\n",
        "    TT = tf.convert_to_tensor(TT)\n",
        "\n",
        "    reQ = tf.reshape(TT, [-1, self.reward_size]).numpy()\n",
        "    reQ_ext = np.tile(reQ, (w_num, 1))\n",
        "    w_ext = np.tile(tf.expand_dims(w, 2), [1, self.action_size * w_num, 1])\n",
        "    w_ext = tf.reshape(w_ext, [-1, self.reward_size])\n",
        "\n",
        "    # Nhan wT.Q\n",
        "    prod = tf.reshape(tf.reduce_sum(tf.multiply(reQ_ext, w_ext), axis = 1), [-1, self.action_size * w_num])\n",
        "\n",
        "    # Tim chi so lon nhat\n",
        "    inds = np.argmax(prod, axis = 1)\n",
        "    mask = np.zeros(prod.shape)\n",
        "    for i in range(len(inds)):\n",
        "      mask[i, inds[i]] = 1\n",
        "    mask = np.reshape(mask, [-1, 1])\n",
        "\n",
        "    HQ = []\n",
        "    for i in range(len(mask)):\n",
        "      if mask[i] == 1:\n",
        "        HQ.append(reQ_ext[i])\n",
        "    HQ = tf.convert_to_tensor(HQ)\n",
        "    return HQ\n",
        "\n",
        "\n",
        "  def forward(self, state, preference, w_num =1):\n",
        "    \"\"\"\n",
        "    Trả giá trị Q và HQ\n",
        "    \"\"\"\n",
        "    s_num = int(preference.shape[-0]/w_num)\n",
        "    x = np.concatenate((state, preference), axis = 1)\n",
        "\n",
        "    x = tf.convert_to_tensor(np.array(x))\n",
        "    x = self.layer1(X)\n",
        "    x = self.layer2(X)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.layer5(X)\n",
        "    q = tf.reshape(q, [q.shape[0], self.action_size, self.reward_size])\n",
        "    Hq = self.H(q, preference, s_num, w_num)\n",
        "    return Hq, q"
      ],
      "metadata": {
        "id": "_szaqtLjbU7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZkB9KZ7-cGBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}